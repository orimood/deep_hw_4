{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Generative Models with GAN/cGAN (CTGAN Fixes)\n",
    "\n",
    "**Key Changes from Original:**\n",
    "- PAC = 10 (packing for discriminator)\n",
    "- HIDDEN_DIM = 256 (larger networks)\n",
    "- BATCH_SIZE = 500 (divisible by PAC)\n",
    "- N_CRITIC = 1 (not 5 like WGAN-GP)\n",
    "- Same learning rate for G and D\n",
    "- Residual blocks in Generator\n",
    "- Log transform for long-tail features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (32561, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.0</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646.0</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721.0</td>\n",
       "      <td>11th</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28.0</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409.0</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age         workclass    fnlwgt  education  education-num  \\\n",
       "0  39.0         State-gov   77516.0  Bachelors           13.0   \n",
       "1  50.0  Self-emp-not-inc   83311.0  Bachelors           13.0   \n",
       "2  38.0           Private  215646.0    HS-grad            9.0   \n",
       "3  53.0           Private  234721.0       11th            7.0   \n",
       "4  28.0           Private  338409.0  Bachelors           13.0   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0        2174.0           0.0            40.0  United-States  <=50K  \n",
       "1           0.0           0.0            13.0  United-States  <=50K  \n",
       "2           0.0           0.0            40.0  United-States  <=50K  \n",
       "3           0.0           0.0            40.0  United-States  <=50K  \n",
       "4           0.0           0.0            40.0           Cuba  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Adult dataset from ARFF file\n",
    "data, meta = arff.loadarff('adult.arff')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Decode byte strings\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].str.decode('utf-8')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values ('?') per column:\n",
      "  workclass: 1836 (5.64%)\n",
      "  occupation: 1843 (5.66%)\n",
      "  native-country: 583 (1.79%)\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values ('?') per column:\")\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        missing_count = (df[col] == '?').sum()\n",
    "        if missing_count > 0:\n",
    "            print(f\"  {col}: {missing_count} ({missing_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continuous features (5): ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
      "Categorical features (9): ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country', 'education-num']\n",
      "Log transform features: ['capital-gain', 'capital-loss']\n",
      "Target: income\n"
     ]
    }
   ],
   "source": [
    "# Define feature types\n",
    "# NOTE: education-num moved to categorical because it's discrete\n",
    "CONTINUOUS_COLS = ['age', 'fnlwgt', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "CATEGORICAL_COLS = ['workclass', 'education', 'marital-status', 'occupation',\n",
    "                    'relationship', 'race', 'sex', 'native-country', 'education-num']\n",
    "TARGET_COL = 'income'\n",
    "\n",
    "# Features that need log transform (long-tail distributions)\n",
    "LOG_TRANSFORM_COLS = ['capital-gain', 'capital-loss']\n",
    "\n",
    "print(f\"Continuous features ({len(CONTINUOUS_COLS)}): {CONTINUOUS_COLS}\")\n",
    "print(f\"Categorical features ({len(CATEGORICAL_COLS)}): {CATEGORICAL_COLS}\")\n",
    "print(f\"Log transform features: {LOG_TRANSFORM_COLS}\")\n",
    "print(f\"Target: {TARGET_COL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VGM Transformer with Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "class VGMTransformer:\n",
    "    \"\"\"VGM transformation with log transform for long-tail and zero/peak inflation.\"\"\"\n",
    "\n",
    "    def __init__(self, n_modes=5, zero_inflated_cols=None, peak_inflated_cols=None, log_transform_cols=None):\n",
    "        self.n_modes = n_modes\n",
    "        self.zero_inflated_cols = zero_inflated_cols or []\n",
    "        self.peak_inflated_cols = peak_inflated_cols or {}\n",
    "        self.log_transform_cols = log_transform_cols or []\n",
    "        self.gmms = {}\n",
    "        self.fitted = False\n",
    "\n",
    "    def _apply_log(self, values, col):\n",
    "        \"\"\"Apply log transform if needed.\"\"\"\n",
    "        if col in self.log_transform_cols:\n",
    "            return np.log1p(np.abs(values))  # log(1 + |x|) to handle zeros\n",
    "        return values\n",
    "\n",
    "    def fit(self, df, continuous_cols):\n",
    "        \"\"\"Fit GMM to each continuous column.\"\"\"\n",
    "        for col in continuous_cols:\n",
    "            values = df[col].values.copy()\n",
    "            values = self._apply_log(values, col)\n",
    "\n",
    "            if col in self.zero_inflated_cols:\n",
    "                nonzero_mask = df[col].values != 0\n",
    "                if nonzero_mask.sum() > self.n_modes:\n",
    "                    fit_values = values[nonzero_mask].reshape(-1, 1)\n",
    "                else:\n",
    "                    fit_values = values.reshape(-1, 1)\n",
    "            elif col in self.peak_inflated_cols:\n",
    "                peak_val = self.peak_inflated_cols[col]\n",
    "                nonpeak_mask = df[col].values != peak_val\n",
    "                if nonpeak_mask.sum() > self.n_modes:\n",
    "                    fit_values = values[nonpeak_mask].reshape(-1, 1)\n",
    "                else:\n",
    "                    fit_values = values.reshape(-1, 1)\n",
    "            else:\n",
    "                fit_values = values.reshape(-1, 1)\n",
    "\n",
    "            gmm = GaussianMixture(n_components=self.n_modes, random_state=42, covariance_type='full')\n",
    "            gmm.fit(fit_values)\n",
    "            self.gmms[col] = gmm\n",
    "\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, df, continuous_cols):\n",
    "        \"\"\"Transform continuous features.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"VGMTransformer must be fitted first\")\n",
    "\n",
    "        results = []\n",
    "        for col in continuous_cols:\n",
    "            gmm = self.gmms[col]\n",
    "            original_values = df[col].values\n",
    "            values = self._apply_log(original_values.copy(), col)\n",
    "\n",
    "            if col in self.zero_inflated_cols:\n",
    "                is_special = (original_values == 0).astype(np.float32).reshape(-1, 1)\n",
    "                values_for_gmm = np.where(original_values == 0, 1e-6, values).reshape(-1, 1)\n",
    "            elif col in self.peak_inflated_cols:\n",
    "                peak_val = self.peak_inflated_cols[col]\n",
    "                is_special = (original_values == peak_val).astype(np.float32).reshape(-1, 1)\n",
    "                values_for_gmm = np.where(original_values == peak_val, values.mean(), values).reshape(-1, 1)\n",
    "            else:\n",
    "                is_special = None\n",
    "                values_for_gmm = values.reshape(-1, 1)\n",
    "\n",
    "            # Get mode probabilities\n",
    "            mode_probs = gmm.predict_proba(values_for_gmm)\n",
    "            modes = gmm.predict(values_for_gmm)\n",
    "\n",
    "            # Normalize values per mode\n",
    "            means = gmm.means_.flatten()\n",
    "            stds = np.sqrt(gmm.covariances_.flatten())\n",
    "            normalized = (values_for_gmm.flatten() - means[modes]) / (4 * stds[modes] + 1e-8)\n",
    "            normalized = np.clip(normalized, -0.99, 0.99).reshape(-1, 1)\n",
    "\n",
    "            # Create mode one-hot\n",
    "            mode_onehot = np.zeros((len(values), self.n_modes))\n",
    "            mode_onehot[np.arange(len(values)), modes] = 1\n",
    "\n",
    "            if is_special is not None:\n",
    "                results.append(np.hstack([is_special, normalized, mode_onehot]))\n",
    "            else:\n",
    "                results.append(np.hstack([normalized, mode_onehot]))\n",
    "\n",
    "        return np.hstack(results) if results else np.array([]).reshape(len(df), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    \"\"\"Preprocessor with zero-inflation, peak-inflation, and log transform.\"\"\"\n",
    "\n",
    "    def __init__(self, continuous_cols, categorical_cols, target_col,\n",
    "                 n_modes=5, rare_threshold=0.01):\n",
    "        self.continuous_cols = continuous_cols\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.target_col = target_col\n",
    "        self.n_modes = n_modes\n",
    "        self.rare_threshold = rare_threshold\n",
    "\n",
    "        self.zero_inflated_cols = ['capital-gain', 'capital-loss']\n",
    "        self.peak_inflated_cols = {'hours-per-week': 40.0}\n",
    "        self.log_transform_cols = ['capital-gain', 'capital-loss']\n",
    "\n",
    "        self.vgm = VGMTransformer(\n",
    "            n_modes=n_modes,\n",
    "            zero_inflated_cols=self.zero_inflated_cols,\n",
    "            peak_inflated_cols=self.peak_inflated_cols,\n",
    "            log_transform_cols=self.log_transform_cols\n",
    "        )\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.category_mappings = {}\n",
    "        self.category_dims = {}\n",
    "        self.mode_values = {}\n",
    "        self.rare_categories = {}\n",
    "\n",
    "    def _group_rare_categories(self, df):\n",
    "        df = df.copy()\n",
    "        for col in self.categorical_cols:\n",
    "            if col == 'education-num':\n",
    "                df[col] = df[col].astype(str)\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            rare_cats = freq[freq < self.rare_threshold].index.tolist()\n",
    "            self.rare_categories[col] = rare_cats\n",
    "            if rare_cats:\n",
    "                df[col] = df[col].replace(rare_cats, 'Other')\n",
    "        return df\n",
    "\n",
    "    def _apply_rare_grouping(self, df):\n",
    "        df = df.copy()\n",
    "        for col in self.categorical_cols:\n",
    "            if col == 'education-num':\n",
    "                df[col] = df[col].astype(str)\n",
    "            if col in self.rare_categories:\n",
    "                df[col] = df[col].replace(self.rare_categories[col], 'Other')\n",
    "        return df\n",
    "\n",
    "    def fit(self, df):\n",
    "        df = df.copy()\n",
    "        if 'education-num' in self.categorical_cols:\n",
    "            df['education-num'] = df['education-num'].astype(str)\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if df[col].dtype == object or col == 'education-num':\n",
    "                valid_values = df[df[col] != '?'][col]\n",
    "                if len(valid_values) > 0:\n",
    "                    self.mode_values[col] = valid_values.mode()[0]\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if col in self.mode_values:\n",
    "                df[col] = df[col].replace('?', self.mode_values[col])\n",
    "\n",
    "        df = self._group_rare_categories(df)\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            unique_vals = sorted(df[col].unique())\n",
    "            self.category_mappings[col] = {v: i for i, v in enumerate(unique_vals)}\n",
    "            self.category_dims[col] = len(unique_vals)\n",
    "\n",
    "        self.vgm.fit(df, self.continuous_cols)\n",
    "        self.label_encoder.fit(df[self.target_col])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        if 'education-num' in self.categorical_cols:\n",
    "            df['education-num'] = df['education-num'].astype(str)\n",
    "\n",
    "        for col in self.categorical_cols:\n",
    "            if col in self.mode_values:\n",
    "                df[col] = df[col].replace('?', self.mode_values[col])\n",
    "\n",
    "        df = self._apply_rare_grouping(df)\n",
    "\n",
    "        continuous_transformed = self.vgm.transform(df, self.continuous_cols)\n",
    "\n",
    "        categorical_arrays = []\n",
    "        for col in self.categorical_cols:\n",
    "            n_categories = self.category_dims[col]\n",
    "            onehot = np.zeros((len(df), n_categories))\n",
    "            for i, val in enumerate(df[col]):\n",
    "                if val in self.category_mappings[col]:\n",
    "                    onehot[i, self.category_mappings[col][val]] = 1\n",
    "                else:\n",
    "                    if 'Other' in self.category_mappings[col]:\n",
    "                        onehot[i, self.category_mappings[col]['Other']] = 1\n",
    "            categorical_arrays.append(onehot)\n",
    "\n",
    "        categorical_transformed = np.hstack(categorical_arrays) if categorical_arrays else np.array([]).reshape(len(df), 0)\n",
    "\n",
    "        X = np.hstack([continuous_transformed, categorical_transformed])\n",
    "        y = self.label_encoder.transform(df[self.target_col])\n",
    "\n",
    "        return X.astype(np.float32), y\n",
    "\n",
    "    def get_output_dim(self):\n",
    "        return self.get_continuous_dim() + sum(self.category_dims.values())\n",
    "\n",
    "    def get_continuous_dim(self):\n",
    "        dim = 0\n",
    "        for col in self.continuous_cols:\n",
    "            if col in self.zero_inflated_cols or col in self.peak_inflated_cols:\n",
    "                dim += 2 + self.n_modes\n",
    "            else:\n",
    "                dim += 1 + self.n_modes\n",
    "        return dim\n",
    "\n",
    "    def get_categorical_dims(self):\n",
    "        return [self.category_dims[col] for col in self.categorical_cols]\n",
    "\n",
    "    def get_n_modes(self):\n",
    "        return self.n_modes\n",
    "\n",
    "    def get_feature_structure(self):\n",
    "        structure = []\n",
    "        for col in self.continuous_cols:\n",
    "            if col in self.zero_inflated_cols:\n",
    "                structure.append({'name': col, 'type': 'zero_inflated', 'dim': 2 + self.n_modes, 'special_value': 0})\n",
    "            elif col in self.peak_inflated_cols:\n",
    "                structure.append({'name': col, 'type': 'peak_inflated', 'dim': 2 + self.n_modes, 'special_value': self.peak_inflated_cols[col]})\n",
    "            else:\n",
    "                structure.append({'name': col, 'type': 'regular', 'dim': 1 + self.n_modes})\n",
    "        return structure\n",
    "\n",
    "    def get_raw_continuous_dim(self):\n",
    "        return len(self.continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total feature dimension: 105\n",
      "VGM continuous dimension: 33\n",
      "Categorical dimensions: [7, 15, 7, 13, 6, 4, 2, 3, 15]\n"
     ]
    }
   ],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(\n",
    "    CONTINUOUS_COLS, \n",
    "    CATEGORICAL_COLS, \n",
    "    TARGET_COL,\n",
    "    n_modes=5,\n",
    "    rare_threshold=0.01\n",
    ")\n",
    "preprocessor.fit(df)\n",
    "\n",
    "print(f\"Total feature dimension: {preprocessor.get_output_dim()}\")\n",
    "print(f\"VGM continuous dimension: {preprocessor.get_continuous_dim()}\")\n",
    "print(f\"Categorical dimensions: {preprocessor.get_categorical_dims()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 26048 samples\n",
      "Test set: 6513 samples\n",
      "Label distribution (train): [0.75917537 0.24082463]\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(df, seed=42, test_size=0.2):\n",
    "    \"\"\"Prepare train and test data with stratified split.\"\"\"\n",
    "    train_df, test_df = train_test_split(\n",
    "        df, test_size=test_size, random_state=seed, stratify=df[TARGET_COL]\n",
    "    )\n",
    "    \n",
    "    preprocessor_local = DataPreprocessor(CONTINUOUS_COLS, CATEGORICAL_COLS, TARGET_COL)\n",
    "    preprocessor_local.fit(train_df)\n",
    "    \n",
    "    X_train, y_train = preprocessor_local.transform(train_df)\n",
    "    X_test, y_test = preprocessor_local.transform(test_df)\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    print(f\"Label distribution (train): {np.bincount(y_train) / len(y_train)}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor_local\n",
    "\n",
    "X_train, X_test, y_train, y_test, preprocessor = prepare_data(df, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Definitions with CTGAN-style Architecture\n",
    "\n",
    "**Key CTGAN features:**\n",
    "- PAC (packing) = 10 samples for discriminator\n",
    "- Residual blocks in Generator\n",
    "- Larger hidden dimensions (256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gumbel_softmax(logits, temperature=0.2, hard=True):\n",
    "    \"\"\"Gumbel-Softmax with straight-through gradient.\"\"\"\n",
    "    gumbels = -torch.log(-torch.log(torch.rand_like(logits) + 1e-20) + 1e-20)\n",
    "    y_soft = torch.softmax((logits + gumbels) / temperature, dim=-1)\n",
    "    if hard:\n",
    "        index = y_soft.max(dim=-1, keepdim=True)[1]\n",
    "        y_hard = torch.zeros_like(logits).scatter_(-1, index, 1.0)\n",
    "        return y_hard - y_soft.detach() + y_soft\n",
    "    return y_soft\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block for Generator (CTGAN-style).\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.bn2 = nn.BatchNorm1d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.bn2(self.fc2(x))\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"CTGAN-style Generator with residual blocks and zero/peak inflation.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, feature_structure, categorical_dims,\n",
    "                 hidden_dim=256, temperature=0.2, n_modes=5):\n",
    "        super().__init__()\n",
    "        self.feature_structure = feature_structure\n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.temperature = temperature\n",
    "        self.n_modes = n_modes\n",
    "\n",
    "        self.continuous_output_dim = sum(f['dim'] for f in feature_structure)\n",
    "        output_dim = self.continuous_output_dim + sum(categorical_dims)\n",
    "\n",
    "        # CTGAN-style: fc -> residual blocks -> fc\n",
    "        self.fc_input = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(hidden_dim),\n",
    "            ResidualBlock(hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.fc_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, hard=True):\n",
    "        x = F.relu(self.bn_input(self.fc_input(z)))\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.fc_output(x)\n",
    "\n",
    "        outputs = []\n",
    "        pos = 0\n",
    "\n",
    "        for feat in self.feature_structure:\n",
    "            if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "                is_special_logit = x[:, pos:pos+1]\n",
    "                is_special_prob = torch.sigmoid(is_special_logit)\n",
    "                pos += 1\n",
    "\n",
    "                value = torch.tanh(x[:, pos:pos+1])\n",
    "                pos += 1\n",
    "\n",
    "                mode_logits = x[:, pos:pos+self.n_modes]\n",
    "                mode_onehot = gumbel_softmax(mode_logits, self.temperature, hard)\n",
    "                pos += self.n_modes\n",
    "\n",
    "                if hard:\n",
    "                    is_special_hard = (is_special_prob > 0.5).float()\n",
    "                    is_special_out = is_special_hard - is_special_prob.detach() + is_special_prob\n",
    "                else:\n",
    "                    is_special_out = is_special_prob\n",
    "\n",
    "                outputs.extend([is_special_out, value, mode_onehot])\n",
    "            else:\n",
    "                value = torch.tanh(x[:, pos:pos+1])\n",
    "                pos += 1\n",
    "\n",
    "                mode_logits = x[:, pos:pos+self.n_modes]\n",
    "                mode_onehot = gumbel_softmax(mode_logits, self.temperature, hard)\n",
    "                pos += self.n_modes\n",
    "\n",
    "                outputs.extend([value, mode_onehot])\n",
    "\n",
    "        for dim in self.categorical_dims:\n",
    "            cat_logits = x[:, pos:pos+dim]\n",
    "            cat_samples = gumbel_softmax(cat_logits, self.temperature, hard)\n",
    "            outputs.append(cat_samples)\n",
    "            pos += dim\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"CTGAN-style Discriminator with PAC (packing).\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim=256, pac=10, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.pac = pac\n",
    "        self.pacdim = input_dim * pac  # Concatenate pac samples\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.pacdim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape for PAC: (batch_size, input_dim) -> (batch_size/pac, input_dim*pac)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size // self.pac, self.pacdim)  # Changed from view to reshape\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "def gradient_penalty(discriminator, real_data, fake_data, device, pac=10):\n",
    "    \"\"\"Compute gradient penalty for WGAN-GP with PAC.\"\"\"\n",
    "    batch_size = real_data.size(0)\n",
    "    alpha = torch.rand(batch_size // pac, 1, device=device)\n",
    "    alpha = alpha.repeat(1, pac).reshape(-1, 1).expand_as(real_data)  # Changed view to reshape\n",
    "    \n",
    "    interpolated = (alpha * real_data + (1 - alpha) * fake_data).requires_grad_(True)\n",
    "    d_out = discriminator(interpolated)\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_out, inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_out),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    grads = grads.reshape(batch_size // pac, -1)  # Changed from view to reshape\n",
    "    return ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "\n",
    "def correlation_loss(real, fake, feature_structure, n_modes):\n",
    "    \"\"\"Correlation loss between real and fake continuous features.\"\"\"\n",
    "    real_values = []\n",
    "    fake_values = []\n",
    "    pos = 0\n",
    "\n",
    "    for feat in feature_structure:\n",
    "        if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "            real_values.append(real[:, pos + 1:pos + 2])\n",
    "            fake_values.append(fake[:, pos + 1:pos + 2])\n",
    "            pos += 2 + n_modes\n",
    "        else:\n",
    "            real_values.append(real[:, pos:pos + 1])\n",
    "            fake_values.append(fake[:, pos:pos + 1])\n",
    "            pos += 1 + n_modes\n",
    "\n",
    "    real_cont = torch.cat(real_values, dim=1)\n",
    "    fake_cont = torch.cat(fake_values, dim=1)\n",
    "\n",
    "    real_std = (real_cont - real_cont.mean(0)) / (real_cont.std(0) + 1e-8)\n",
    "    fake_std = (fake_cont - fake_cont.mean(0)) / (fake_cont.std(0) + 1e-8)\n",
    "\n",
    "    r_corr = torch.mm(real_std.t(), real_std) / real_std.size(0)\n",
    "    f_corr = torch.mm(fake_std.t(), fake_std) / fake_std.size(0)\n",
    "\n",
    "    return F.mse_loss(f_corr, r_corr)\n",
    "\n",
    "\n",
    "def special_proportion_loss(real, fake, feature_structure, n_modes):\n",
    "    \"\"\"Loss to match proportion of special values.\"\"\"\n",
    "    loss = 0\n",
    "    pos = 0\n",
    "    count = 0\n",
    "\n",
    "    for feat in feature_structure:\n",
    "        if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "            real_is_special = real[:, pos]\n",
    "            fake_is_special = fake[:, pos]\n",
    "            loss += (real_is_special.mean() - fake_is_special.mean()) ** 2\n",
    "            count += 1\n",
    "            pos += 2 + n_modes\n",
    "        else:\n",
    "            pos += 1 + n_modes\n",
    "\n",
    "    return loss / max(count, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conditional GAN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalGenerator(nn.Module):\n",
    "    \"\"\"CTGAN-style Conditional Generator.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_dim, num_classes, feature_structure, categorical_dims,\n",
    "                 hidden_dim=256, temperature=0.2, n_modes=5):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_structure = feature_structure\n",
    "        self.categorical_dims = categorical_dims\n",
    "        self.temperature = temperature\n",
    "        self.n_modes = n_modes\n",
    "\n",
    "        input_dim = latent_dim + num_classes\n",
    "        self.continuous_output_dim = sum(f['dim'] for f in feature_structure)\n",
    "        output_dim = self.continuous_output_dim + sum(categorical_dims)\n",
    "\n",
    "        self.fc_input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn_input = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.res_blocks = nn.Sequential(\n",
    "            ResidualBlock(hidden_dim),\n",
    "            ResidualBlock(hidden_dim),\n",
    "        )\n",
    "        \n",
    "        self.fc_output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, z, labels, hard=True):\n",
    "        if labels.dim() == 1:\n",
    "            labels_onehot = torch.zeros(labels.size(0), self.num_classes, device=z.device)\n",
    "            labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "        else:\n",
    "            labels_onehot = labels\n",
    "\n",
    "        x = torch.cat([z, labels_onehot], dim=1)\n",
    "        x = F.relu(self.bn_input(self.fc_input(x)))\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.fc_output(x)\n",
    "\n",
    "        outputs = []\n",
    "        pos = 0\n",
    "\n",
    "        for feat in self.feature_structure:\n",
    "            if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "                is_special_logit = x[:, pos:pos+1]\n",
    "                is_special_prob = torch.sigmoid(is_special_logit)\n",
    "                pos += 1\n",
    "\n",
    "                value = torch.tanh(x[:, pos:pos+1])\n",
    "                pos += 1\n",
    "\n",
    "                mode_logits = x[:, pos:pos+self.n_modes]\n",
    "                mode_onehot = gumbel_softmax(mode_logits, self.temperature, hard)\n",
    "                pos += self.n_modes\n",
    "\n",
    "                if hard:\n",
    "                    is_special_hard = (is_special_prob > 0.5).float()\n",
    "                    is_special_out = is_special_hard - is_special_prob.detach() + is_special_prob\n",
    "                else:\n",
    "                    is_special_out = is_special_prob\n",
    "\n",
    "                outputs.extend([is_special_out, value, mode_onehot])\n",
    "            else:\n",
    "                value = torch.tanh(x[:, pos:pos+1])\n",
    "                pos += 1\n",
    "\n",
    "                mode_logits = x[:, pos:pos+self.n_modes]\n",
    "                mode_onehot = gumbel_softmax(mode_logits, self.temperature, hard)\n",
    "                pos += self.n_modes\n",
    "\n",
    "                outputs.extend([value, mode_onehot])\n",
    "\n",
    "        for dim in self.categorical_dims:\n",
    "            cat_logits = x[:, pos:pos+dim]\n",
    "            cat_samples = gumbel_softmax(cat_logits, self.temperature, hard)\n",
    "            outputs.append(cat_samples)\n",
    "            pos += dim\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    \"\"\"CTGAN-style Conditional Discriminator with PAC.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=256, pac=10, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.pac = pac\n",
    "        self.num_classes = num_classes\n",
    "        self.pacdim = (input_dim + num_classes) * pac\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.pacdim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        if labels.dim() == 1:\n",
    "            labels_onehot = torch.zeros(labels.size(0), self.num_classes, device=x.device)\n",
    "            labels_onehot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "        else:\n",
    "            labels_onehot = labels\n",
    "\n",
    "        x = torch.cat([x, labels_onehot], dim=1)\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size // self.pac, self.pacdim)  # Changed from view to reshape\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "def gradient_penalty_cgan(discriminator, real_data, fake_data, labels, device, pac=10):\n",
    "    \"\"\"Gradient penalty for cGAN with PAC.\"\"\"\n",
    "    batch_size = real_data.size(0)\n",
    "    alpha = torch.rand(batch_size // pac, 1, device=device)\n",
    "    alpha = alpha.repeat(1, pac).reshape(-1, 1).expand_as(real_data)  # Changed view to reshape\n",
    "    \n",
    "    interpolated = (alpha * real_data + (1 - alpha) * fake_data).requires_grad_(True)\n",
    "    d_out = discriminator(interpolated, labels)\n",
    "\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_out, inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(d_out),\n",
    "        create_graph=True, retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    grads = grads.reshape(batch_size // pac, -1)  # Changed from view to reshape\n",
    "    return ((grads.norm(2, dim=1) - 1) ** 2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Functions with CTGAN Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(X_train, preprocessor, latent_dim=128, hidden_dim=256,\n",
    "              batch_size=500, epochs=300, lr=0.0002,\n",
    "              n_critic=1, lambda_gp=10, lambda_corr=0.1, lambda_special=0.5,\n",
    "              temperature=0.2, dropout=0.5, pac=10, seed=42,\n",
    "              save_dir='plots_v2/gan'):\n",
    "    \"\"\"\n",
    "    Train WGAN-GP with CTGAN-style architecture.\n",
    "    \n",
    "    Key CTGAN differences:\n",
    "    - PAC = 10 (packing)\n",
    "    - n_critic = 1 (not 5)\n",
    "    - Same lr for G and D\n",
    "    - Larger hidden_dim = 256\n",
    "    - batch_size = 500\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    feature_structure = preprocessor.get_feature_structure()\n",
    "    n_modes = preprocessor.get_n_modes()\n",
    "    categorical_dims = preprocessor.get_categorical_dims()\n",
    "    data_dim = preprocessor.get_output_dim()\n",
    "\n",
    "    generator = Generator(\n",
    "        latent_dim=latent_dim,\n",
    "        feature_structure=feature_structure,\n",
    "        categorical_dims=categorical_dims,\n",
    "        hidden_dim=hidden_dim,\n",
    "        temperature=temperature,\n",
    "        n_modes=n_modes\n",
    "    ).to(device)\n",
    "\n",
    "    discriminator = Discriminator(\n",
    "        input_dim=data_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        pac=pac,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    # Ensure batch_size is divisible by pac\n",
    "    effective_batch_size = (batch_size // pac) * pac\n",
    "    \n",
    "    dataset = TensorDataset(torch.FloatTensor(X_train))\n",
    "    dataloader = DataLoader(dataset, batch_size=effective_batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training GAN (CTGAN-style)\"):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_idx, (real_data,) in enumerate(dataloader):\n",
    "            batch_size_actual = real_data.size(0)\n",
    "            real_data = real_data.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_d.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "                fake_data = generator(z)\n",
    "\n",
    "                d_real = discriminator(real_data)\n",
    "                d_fake = discriminator(fake_data.detach())\n",
    "                \n",
    "                gp = gradient_penalty(discriminator, real_data, fake_data.detach(), device, pac)\n",
    "                d_loss = d_fake.mean() - d_real.mean() + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_data = generator(z)\n",
    "            d_fake = discriminator(fake_data)\n",
    "\n",
    "            g_loss_wgan = -d_fake.mean()\n",
    "            corr_loss = correlation_loss(real_data, fake_data, feature_structure, n_modes)\n",
    "            special_loss = special_proportion_loss(real_data, fake_data, feature_structure, n_modes)\n",
    "\n",
    "            g_loss = g_loss_wgan + lambda_corr * corr_loss + lambda_special * special_loss\n",
    "            g_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "            optimizer_g.step()\n",
    "\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        d_losses.append(epoch_d_loss / n_batches)\n",
    "        g_losses.append(epoch_g_loss / n_batches)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] D_loss: {d_losses[-1]:.4f} G_loss: {g_losses[-1]:.4f}\")\n",
    "\n",
    "    return generator, g_losses, d_losses\n",
    "\n",
    "\n",
    "def train_cgan(X_train, y_train, preprocessor, latent_dim=128, hidden_dim=256,\n",
    "               batch_size=500, epochs=300, lr=0.0002,\n",
    "               n_critic=1, lambda_gp=10, lambda_corr=0.1, lambda_special=0.5,\n",
    "               temperature=0.2, dropout=0.5, pac=10, seed=42,\n",
    "               save_dir='plots_v2/cgan'):\n",
    "    \"\"\"\n",
    "    Train Conditional WGAN-GP with CTGAN-style architecture.\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    feature_structure = preprocessor.get_feature_structure()\n",
    "    n_modes = preprocessor.get_n_modes()\n",
    "    categorical_dims = preprocessor.get_categorical_dims()\n",
    "    data_dim = preprocessor.get_output_dim()\n",
    "    num_classes = len(np.unique(y_train))\n",
    "\n",
    "    generator = ConditionalGenerator(\n",
    "        latent_dim=latent_dim,\n",
    "        num_classes=num_classes,\n",
    "        feature_structure=feature_structure,\n",
    "        categorical_dims=categorical_dims,\n",
    "        hidden_dim=hidden_dim,\n",
    "        temperature=temperature,\n",
    "        n_modes=n_modes\n",
    "    ).to(device)\n",
    "\n",
    "    discriminator = ConditionalDiscriminator(\n",
    "        input_dim=data_dim,\n",
    "        num_classes=num_classes,\n",
    "        hidden_dim=hidden_dim,\n",
    "        pac=pac,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "\n",
    "    effective_batch_size = (batch_size // pac) * pac\n",
    "    \n",
    "    dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    dataloader = DataLoader(dataset, batch_size=effective_batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training cGAN (CTGAN-style)\"):\n",
    "        epoch_g_loss = 0\n",
    "        epoch_d_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch_idx, (real_data, labels) in enumerate(dataloader):\n",
    "            batch_size_actual = real_data.size(0)\n",
    "            real_data = real_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Train Discriminator\n",
    "            for _ in range(n_critic):\n",
    "                optimizer_d.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "                fake_data = generator(z, labels)\n",
    "\n",
    "                d_real = discriminator(real_data, labels)\n",
    "                d_fake = discriminator(fake_data.detach(), labels)\n",
    "                \n",
    "                gp = gradient_penalty_cgan(discriminator, real_data, fake_data.detach(), labels, device, pac)\n",
    "                d_loss = d_fake.mean() - d_real.mean() + lambda_gp * gp\n",
    "                \n",
    "                d_loss.backward()\n",
    "                optimizer_d.step()\n",
    "\n",
    "            # Train Generator\n",
    "            optimizer_g.zero_grad()\n",
    "            \n",
    "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
    "            fake_data = generator(z, labels)\n",
    "            d_fake = discriminator(fake_data, labels)\n",
    "\n",
    "            g_loss_wgan = -d_fake.mean()\n",
    "            corr_loss = correlation_loss(real_data, fake_data, feature_structure, n_modes)\n",
    "            special_loss = special_proportion_loss(real_data, fake_data, feature_structure, n_modes)\n",
    "\n",
    "            g_loss = g_loss_wgan + lambda_corr * corr_loss + lambda_special * special_loss\n",
    "            g_loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "            optimizer_g.step()\n",
    "\n",
    "            epoch_d_loss += d_loss.item()\n",
    "            epoch_g_loss += g_loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        d_losses.append(epoch_d_loss / n_batches)\n",
    "        g_losses.append(epoch_g_loss / n_batches)\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] D_loss: {d_losses[-1]:.4f} G_loss: {g_losses[-1]:.4f}\")\n",
    "\n",
    "    return generator, g_losses, d_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(generator, n_samples, latent_dim, device):\n",
    "    \"\"\"Generate synthetic data using trained GAN.\"\"\"\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(n_samples, latent_dim, device=device)\n",
    "        synthetic_data = generator(z, hard=True)\n",
    "    return synthetic_data.cpu().numpy()\n",
    "\n",
    "\n",
    "def generate_conditional_synthetic_data(generator, n_samples, latent_dim, label_ratios, device):\n",
    "    \"\"\"Generate synthetic data using trained cGAN with specified label ratios.\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    samples_per_class = (np.array(label_ratios) * n_samples).astype(int)\n",
    "    samples_per_class[-1] = n_samples - samples_per_class[:-1].sum()\n",
    "    \n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for label, n in enumerate(samples_per_class):\n",
    "            if n > 0:\n",
    "                z = torch.randn(n, latent_dim, device=device)\n",
    "                labels = torch.full((n,), label, dtype=torch.long, device=device)\n",
    "                synthetic = generator(z, labels, hard=True)\n",
    "                all_data.append(synthetic.cpu().numpy())\n",
    "                all_labels.append(np.full(n, label))\n",
    "    \n",
    "    return np.vstack(all_data), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detection_metric(X_real, X_synthetic, n_folds=4, seed=42):\n",
    "    \"\"\"Compute detection metric using Random Forest.\"\"\"\n",
    "    y_real = np.zeros(len(X_real))\n",
    "    y_synthetic = np.ones(len(X_synthetic))\n",
    "    \n",
    "    X_combined = np.vstack([X_real, X_synthetic])\n",
    "    y_combined = np.concatenate([y_real, y_synthetic])\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    auc_scores = []\n",
    "    \n",
    "    for train_idx, test_idx in kfold.split(X_combined, y_combined):\n",
    "        X_train_fold, X_test_fold = X_combined[train_idx], X_combined[test_idx]\n",
    "        y_train_fold, y_test_fold = y_combined[train_idx], y_combined[test_idx]\n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=seed, n_jobs=-1)\n",
    "        rf.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        y_pred_proba = rf.predict_proba(X_test_fold)[:, 1]\n",
    "        auc = roc_auc_score(y_test_fold, y_pred_proba)\n",
    "        auc_scores.append(auc)\n",
    "    \n",
    "    return np.mean(auc_scores), np.std(auc_scores)\n",
    "\n",
    "\n",
    "def compute_efficacy_metric(X_train_real, y_train_real, X_synthetic, y_synthetic, \n",
    "                           X_test, y_test, seed=42):\n",
    "    \"\"\"Compute efficacy metric.\"\"\"\n",
    "    rf_real = RandomForestClassifier(n_estimators=100, random_state=seed, n_jobs=-1)\n",
    "    rf_real.fit(X_train_real, y_train_real)\n",
    "    y_pred_proba_real = rf_real.predict_proba(X_test)[:, 1]\n",
    "    auc_real = roc_auc_score(y_test, y_pred_proba_real)\n",
    "    \n",
    "    rf_synthetic = RandomForestClassifier(n_estimators=100, random_state=seed, n_jobs=-1)\n",
    "    rf_synthetic.fit(X_synthetic, y_synthetic)\n",
    "    y_pred_proba_synthetic = rf_synthetic.predict_proba(X_test)[:, 1]\n",
    "    auc_synthetic = roc_auc_score(y_test, y_pred_proba_synthetic)\n",
    "    \n",
    "    efficacy_ratio = auc_synthetic / auc_real\n",
    "    \n",
    "    return efficacy_ratio, auc_real, auc_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_losses(g_losses, d_losses, title=\"GAN Training Losses\", save_path=None):\n",
    "    \"\"\"Plot generator and discriminator losses.\"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(g_losses, label='Generator Loss', alpha=0.8)\n",
    "    plt.plot(d_losses, label='Discriminator Loss', alpha=0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_distributions(X_real, X_synthetic, preprocessor, n_features=5,\n",
    "                               title=\"Feature Distributions\", save_path=None):\n",
    "    \"\"\"Plot histograms comparing real vs synthetic.\"\"\"\n",
    "    feature_structure = preprocessor.get_feature_structure()\n",
    "    n_modes = preprocessor.get_n_modes()\n",
    "    n_plots = min(n_features, len(feature_structure))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    pos = 0\n",
    "    for i, feat in enumerate(feature_structure[:n_plots]):\n",
    "        ax = axes[i]\n",
    "\n",
    "        if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "            is_special_real = X_real[:, pos]\n",
    "            is_special_synth = X_synthetic[:, pos]\n",
    "            value_real = X_real[:, pos + 1]\n",
    "            value_synth = X_synthetic[:, pos + 1]\n",
    "\n",
    "            combined_real = np.where(is_special_real > 0.5, -1.5, value_real)\n",
    "            combined_synth = np.where(is_special_synth > 0.5, -1.5, value_synth)\n",
    "\n",
    "            ax.hist(combined_real, bins=40, alpha=0.5, label='Real', density=True, range=(-1.5, 1))\n",
    "            ax.hist(combined_synth, bins=40, alpha=0.5, label='Synthetic', density=True, range=(-1.5, 1))\n",
    "            ax.axvline(x=-1.25, color='red', linestyle='--', linewidth=0.5)\n",
    "\n",
    "            special_prop_real = (is_special_real > 0.5).mean() * 100\n",
    "            special_prop_synth = (is_special_synth > 0.5).mean() * 100\n",
    "            special_name = \"Zeros\" if feat['type'] == 'zero_inflated' else f\"Peak@{feat['special_value']}\"\n",
    "            ax.set_title(f'{feat[\"name\"]}\\n{special_name}: R={special_prop_real:.1f}%, S={special_prop_synth:.1f}%')\n",
    "\n",
    "            pos += 2 + n_modes\n",
    "        else:\n",
    "            ax.hist(X_real[:, pos], bins=30, alpha=0.5, label='Real', density=True)\n",
    "            ax.hist(X_synthetic[:, pos], bins=30, alpha=0.5, label='Synthetic', density=True)\n",
    "            ax.set_title(f'{feat[\"name\"]}')\n",
    "            pos += 1 + n_modes\n",
    "\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if n_plots < 6:\n",
    "        axes[5].axis('off')\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_matrices(X_real, X_synthetic, preprocessor,\n",
    "                             title=\"Correlation Comparison\", save_path=None):\n",
    "    \"\"\"Plot correlation matrices.\"\"\"\n",
    "    feature_structure = preprocessor.get_feature_structure()\n",
    "    n_modes = preprocessor.get_n_modes()\n",
    "\n",
    "    real_values = []\n",
    "    synth_values = []\n",
    "    names = []\n",
    "    pos = 0\n",
    "\n",
    "    for feat in feature_structure:\n",
    "        if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "            real_values.append(X_real[:, pos + 1])\n",
    "            synth_values.append(X_synthetic[:, pos + 1])\n",
    "            pos += 2 + n_modes\n",
    "        else:\n",
    "            real_values.append(X_real[:, pos])\n",
    "            synth_values.append(X_synthetic[:, pos])\n",
    "            pos += 1 + n_modes\n",
    "        names.append(feat['name'])\n",
    "\n",
    "    real_cont = np.column_stack(real_values)\n",
    "    synth_cont = np.column_stack(synth_values)\n",
    "\n",
    "    corr_real = np.corrcoef(real_cont.T)\n",
    "    corr_synth = np.corrcoef(synth_cont.T)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    sns.heatmap(corr_real, ax=axes[0], cmap='coolwarm', center=0,\n",
    "                xticklabels=names, yticklabels=names,\n",
    "                annot=True, fmt='.2f', square=True)\n",
    "    axes[0].set_title('Real Data Correlation')\n",
    "\n",
    "    sns.heatmap(corr_synth, ax=axes[1], cmap='coolwarm', center=0,\n",
    "                xticklabels=names, yticklabels=names,\n",
    "                annot=True, fmt='.2f', square=True)\n",
    "    axes[1].set_title('Synthetic Data Correlation')\n",
    "\n",
    "    corr_diff = corr_real - corr_synth\n",
    "    sns.heatmap(corr_diff, ax=axes[2], cmap='coolwarm', center=0,\n",
    "                xticklabels=names, yticklabels=names,\n",
    "                annot=True, fmt='.2f', square=True)\n",
    "    axes[2].set_title('Difference (Real - Synthetic)')\n",
    "\n",
    "    plt.suptitle(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Configuration (CTGAN Defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CTGAN-STYLE CONFIGURATION\n",
      "============================================================\n",
      "Seeds: [42, 123, 456]\n",
      "Latent dimension: 128\n",
      "Hidden dimension: 256 (CTGAN: 256)\n",
      "Batch size: 500 (CTGAN: 500)\n",
      "Epochs: 300\n",
      "Learning rate (G & D): 0.0002\n",
      "Critic iterations: 1 (CTGAN: 1)\n",
      "PAC (packing): 10 (CTGAN: 10)\n",
      "Gradient penalty lambda: 10\n",
      "Correlation loss lambda: 0.1\n",
      "Special proportion loss lambda: 0.5\n",
      "Gumbel-Softmax temperature: 0.2\n",
      "Discriminator dropout: 0.5\n",
      "VGM modes: 5\n",
      "============================================================\n",
      "\n",
      "KEY CTGAN CHANGES:\n",
      "- PAC = 10 (packing samples for discriminator)\n",
      "- N_CRITIC = 1 (not 5 like WGAN-GP)\n",
      "- Same learning rate for G and D\n",
      "- Larger network (256 hidden)\n",
      "- Residual blocks in Generator\n",
      "- Log transform for capital-gain/loss\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# CTGAN-style Configuration\n",
    "SEEDS = [42, 123, 456]\n",
    "LATENT_DIM = 128\n",
    "HIDDEN_DIM = 256       # CTGAN uses 256 (was 128)\n",
    "BATCH_SIZE = 500       # CTGAN uses 500 (was 128)\n",
    "EPOCHS = 300           # CTGAN uses 300 (was 500)\n",
    "LR = 0.0002            # Same for G and D (CTGAN style)\n",
    "N_CRITIC = 1           # CTGAN uses 1 (was 5)\n",
    "LAMBDA_GP = 10\n",
    "LAMBDA_CORR = 0.1\n",
    "LAMBDA_SPECIAL = 0.5\n",
    "TEMPERATURE = 0.2\n",
    "DROPOUT = 0.5\n",
    "PAC = 10               # CTGAN packing (NEW)\n",
    "N_MODES = 5\n",
    "RARE_THRESHOLD = 0.01\n",
    "\n",
    "os.makedirs('plots_v2/gan', exist_ok=True)\n",
    "os.makedirs('plots_v2/cgan', exist_ok=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CTGAN-STYLE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Seeds: {SEEDS}\")\n",
    "print(f\"Latent dimension: {LATENT_DIM}\")\n",
    "print(f\"Hidden dimension: {HIDDEN_DIM} (CTGAN: 256)\")\n",
    "print(f\"Batch size: {BATCH_SIZE} (CTGAN: 500)\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate (G & D): {LR}\")\n",
    "print(f\"Critic iterations: {N_CRITIC} (CTGAN: 1)\")\n",
    "print(f\"PAC (packing): {PAC} (CTGAN: 10)\")\n",
    "print(f\"Gradient penalty lambda: {LAMBDA_GP}\")\n",
    "print(f\"Correlation loss lambda: {LAMBDA_CORR}\")\n",
    "print(f\"Special proportion loss lambda: {LAMBDA_SPECIAL}\")\n",
    "print(f\"Gumbel-Softmax temperature: {TEMPERATURE}\")\n",
    "print(f\"Discriminator dropout: {DROPOUT}\")\n",
    "print(f\"VGM modes: {N_MODES}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nKEY CTGAN CHANGES:\")\n",
    "print(\"- PAC = 10 (packing samples for discriminator)\")\n",
    "print(\"- N_CRITIC = 1 (not 5 like WGAN-GP)\")\n",
    "print(\"- Same learning rate for G and D\")\n",
    "print(\"- Larger network (256 hidden)\")\n",
    "print(\"- Residual blocks in Generator\")\n",
    "print(\"- Log transform for capital-gain/loss\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running GAN with seed 42\n",
      "============================================================\n",
      "Training set: 26048 samples\n",
      "Test set: 6513 samples\n",
      "Label distribution (train): [0.75917537 0.24082463]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN (CTGAN-style):  17%|        | 50/300 [01:42<08:26,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300] D_loss: 0.0143 G_loss: 0.2034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN (CTGAN-style):  33%|      | 100/300 [03:23<06:40,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/300] D_loss: -0.0357 G_loss: -0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN (CTGAN-style):  50%|     | 150/300 [05:03<04:56,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/300] D_loss: -0.0367 G_loss: 0.3860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN (CTGAN-style):  67%|   | 200/300 [06:44<03:19,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/300] D_loss: -0.0029 G_loss: 0.0211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN (CTGAN-style):  83%| | 250/300 [08:28<01:46,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/300] D_loss: 0.0269 G_loss: -0.0556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training GAN (CTGAN-style):  84%| | 253/300 [08:36<01:35,  2.04s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m X_train, X_test, y_train, y_test, preprocessor_seed = prepare_data(df, seed=seed)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m generator, g_losses, d_losses = \u001b[43mtrain_gan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLATENT_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHIDDEN_DIM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_critic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_CRITIC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_gp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAMBDA_GP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_corr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAMBDA_CORR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_special\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAMBDA_SPECIAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDROPOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpac\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPAC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mplots_v2/gan\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Plot losses\u001b[39;00m\n\u001b[32m     20\u001b[39m plot_training_losses(g_losses, d_losses, \n\u001b[32m     21\u001b[39m                     title=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGAN Training Losses (Seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m                     save_path=\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mplots_v2/gan/losses_seed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mtrain_gan\u001b[39m\u001b[34m(X_train, preprocessor, latent_dim, hidden_dim, batch_size, epochs, lr, n_critic, lambda_gp, lambda_corr, lambda_special, temperature, dropout, pac, seed, save_dir)\u001b[39m\n\u001b[32m     71\u001b[39m     gp = gradient_penalty(discriminator, real_data, fake_data.detach(), device, pac)\n\u001b[32m     72\u001b[39m     d_loss = d_fake.mean() - d_real.mean() + lambda_gp * gp\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m     \u001b[43md_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     optimizer_d.step()\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# Train Generator\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\uni\\deep_hw_4\\venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\uni\\deep_hw_4\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\uni\\deep_hw_4\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run GAN experiment with seed 42\n",
    "seed = 42\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Running GAN with seed {seed}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test, preprocessor_seed = prepare_data(df, seed=seed)\n",
    "\n",
    "generator, g_losses, d_losses = train_gan(\n",
    "    X_train, preprocessor_seed,\n",
    "    latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR,\n",
    "    n_critic=N_CRITIC, lambda_gp=LAMBDA_GP,\n",
    "    lambda_corr=LAMBDA_CORR, lambda_special=LAMBDA_SPECIAL,\n",
    "    temperature=TEMPERATURE, dropout=DROPOUT, pac=PAC,\n",
    "    seed=seed, save_dir=f'plots_v2/gan'\n",
    ")\n",
    "\n",
    "# Plot losses\n",
    "plot_training_losses(g_losses, d_losses, \n",
    "                    title=f\"GAN Training Losses (Seed {seed})\",\n",
    "                    save_path=f'plots_v2/gan/losses_seed{seed}.png')\n",
    "\n",
    "# Generate synthetic data\n",
    "X_synthetic = generate_synthetic_data(generator, len(X_train), LATENT_DIM, device)\n",
    "y_synthetic = y_train.copy()  # For GAN, use same label distribution\n",
    "\n",
    "# === DIAGNOSTIC: Check for issues ===\n",
    "print(\"\\n=== DIAGNOSTIC INFO ===\")\n",
    "print(f\"X_train shape: {X_train.shape}, X_synthetic shape: {X_synthetic.shape}\")\n",
    "print(f\"X_train range: [{X_train.min():.4f}, {X_train.max():.4f}]\")\n",
    "print(f\"X_synthetic range: [{X_synthetic.min():.4f}, {X_synthetic.max():.4f}]\")\n",
    "print(f\"X_train has NaN: {np.isnan(X_train).any()}, X_synthetic has NaN: {np.isnan(X_synthetic).any()}\")\n",
    "\n",
    "# Check categorical feature distributions (start at position 33 after continuous)\n",
    "cont_dim = preprocessor_seed.get_continuous_dim()\n",
    "cat_dims = preprocessor_seed.get_categorical_dims()\n",
    "print(f\"\\nContinuous dim: {cont_dim}, Categorical dims: {cat_dims}\")\n",
    "\n",
    "# Check first categorical feature distribution\n",
    "pos = cont_dim\n",
    "for i, (col, dim) in enumerate(zip(CATEGORICAL_COLS[:3], cat_dims[:3])):\n",
    "    real_cat = X_train[:, pos:pos+dim]\n",
    "    synth_cat = X_synthetic[:, pos:pos+dim]\n",
    "    \n",
    "    # Check if one-hot (should sum to 1)\n",
    "    real_sum = real_cat.sum(axis=1)\n",
    "    synth_sum = synth_cat.sum(axis=1)\n",
    "    \n",
    "    print(f\"\\n{col} (dim={dim}):\")\n",
    "    print(f\"  Real - sum range: [{real_sum.min():.4f}, {real_sum.max():.4f}], mean per cat: {real_cat.mean(axis=0)[:3]}\")\n",
    "    print(f\"  Synth - sum range: [{synth_sum.min():.4f}, {synth_sum.max():.4f}], mean per cat: {synth_cat.mean(axis=0)[:3]}\")\n",
    "    pos += dim\n",
    "\n",
    "# Check mode vectors in continuous features\n",
    "print(\"\\n=== MODE VECTOR CHECK ===\")\n",
    "feature_structure = preprocessor_seed.get_feature_structure()\n",
    "n_modes = preprocessor_seed.get_n_modes()\n",
    "pos = 0\n",
    "for feat in feature_structure[:2]:\n",
    "    if feat['type'] in ['zero_inflated', 'peak_inflated']:\n",
    "        mode_start = pos + 2\n",
    "        mode_end = pos + 2 + n_modes\n",
    "        pos += 2 + n_modes\n",
    "    else:\n",
    "        mode_start = pos + 1\n",
    "        mode_end = pos + 1 + n_modes\n",
    "        pos += 1 + n_modes\n",
    "    \n",
    "    real_modes = X_train[:, mode_start:mode_end]\n",
    "    synth_modes = X_synthetic[:, mode_start:mode_end]\n",
    "    \n",
    "    print(f\"{feat['name']} modes:\")\n",
    "    print(f\"  Real - sum range: [{real_modes.sum(axis=1).min():.4f}, {real_modes.sum(axis=1).max():.4f}]\")\n",
    "    print(f\"  Synth - sum range: [{synth_modes.sum(axis=1).min():.4f}, {synth_modes.sum(axis=1).max():.4f}]\")\n",
    "    print(f\"  Real mode distribution: {real_modes.mean(axis=0)}\")\n",
    "    print(f\"  Synth mode distribution: {synth_modes.mean(axis=0)}\")\n",
    "\n",
    "print(\"=== END DIAGNOSTIC ===\\n\")\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(X_train, X_synthetic, preprocessor_seed,\n",
    "                          title=f\"GAN Feature Distributions (Seed {seed})\",\n",
    "                          save_path=f'plots_v2/gan/features_seed{seed}.png')\n",
    "\n",
    "# Plot correlations\n",
    "plot_correlation_matrices(X_train, X_synthetic, preprocessor_seed,\n",
    "                         title=f\"GAN Correlation Comparison (Seed {seed})\",\n",
    "                         save_path=f'plots_v2/gan/correlation_seed{seed}.png')\n",
    "\n",
    "# Compute metrics\n",
    "detection_auc, detection_std = compute_detection_metric(X_train, X_synthetic, seed=seed)\n",
    "efficacy, auc_real, auc_synth = compute_efficacy_metric(\n",
    "    X_train, y_train, X_synthetic, y_synthetic, X_test, y_test, seed=seed\n",
    ")\n",
    "\n",
    "print(f\"\\nGAN Results (Seed {seed}):\")\n",
    "print(f\"  Detection AUC: {detection_auc:.4f} (+/- {detection_std:.4f})\")\n",
    "print(f\"  Efficacy Ratio: {efficacy:.4f}\")\n",
    "print(f\"  AUC (Real): {auc_real:.4f}, AUC (Synthetic): {auc_synth:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running cGAN with seed 42\n",
      "============================================================\n",
      "Training set: 26048 samples\n",
      "Test set: 6513 samples\n",
      "Label distribution (train): [0.75917537 0.24082463]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cGAN (CTGAN-style):  17%|        | 50/300 [01:47<08:45,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300] D_loss: 0.0609 G_loss: 1.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training cGAN (CTGAN-style):  26%|       | 78/300 [02:48<08:03,  2.18s/it]"
     ]
    }
   ],
   "source": [
    "# Run cGAN experiment with seed 42\n",
    "seed = 42\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Running cGAN with seed {seed}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test, preprocessor_seed = prepare_data(df, seed=seed)\n",
    "\n",
    "cgan_generator, cgan_g_losses, cgan_d_losses = train_cgan(\n",
    "    X_train, y_train, preprocessor_seed,\n",
    "    latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "    batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR,\n",
    "    n_critic=N_CRITIC, lambda_gp=LAMBDA_GP,\n",
    "    lambda_corr=LAMBDA_CORR, lambda_special=LAMBDA_SPECIAL,\n",
    "    temperature=TEMPERATURE, dropout=DROPOUT, pac=PAC,\n",
    "    seed=seed, save_dir=f'plots_v2/cgan'\n",
    ")\n",
    "\n",
    "# Plot losses\n",
    "plot_training_losses(cgan_g_losses, cgan_d_losses,\n",
    "                    title=f\"cGAN Training Losses (Seed {seed})\",\n",
    "                    save_path=f'plots_v2/cgan/losses_seed{seed}.png')\n",
    "\n",
    "# Generate synthetic data with correct label ratios\n",
    "label_ratios = np.bincount(y_train) / len(y_train)\n",
    "X_synthetic_cgan, y_synthetic_cgan = generate_conditional_synthetic_data(\n",
    "    cgan_generator, len(X_train), LATENT_DIM, label_ratios, device\n",
    ")\n",
    "\n",
    "# Plot feature distributions\n",
    "plot_feature_distributions(X_train, X_synthetic_cgan, preprocessor_seed,\n",
    "                          title=f\"cGAN Feature Distributions (Seed {seed})\",\n",
    "                          save_path=f'plots_v2/cgan/features_seed{seed}.png')\n",
    "\n",
    "# Plot correlations\n",
    "plot_correlation_matrices(X_train, X_synthetic_cgan, preprocessor_seed,\n",
    "                         title=f\"cGAN Correlation Comparison (Seed {seed})\",\n",
    "                         save_path=f'plots_v2/cgan/correlation_seed{seed}.png')\n",
    "\n",
    "# Compute metrics\n",
    "cgan_detection_auc, cgan_detection_std = compute_detection_metric(X_train, X_synthetic_cgan, seed=seed)\n",
    "cgan_efficacy, cgan_auc_real, cgan_auc_synth = compute_efficacy_metric(\n",
    "    X_train, y_train, X_synthetic_cgan, y_synthetic_cgan, X_test, y_test, seed=seed\n",
    ")\n",
    "\n",
    "print(f\"\\ncGAN Results (Seed {seed}):\")\n",
    "print(f\"  Detection AUC: {cgan_detection_auc:.4f} (+/- {cgan_detection_std:.4f})\")\n",
    "print(f\"  Efficacy Ratio: {cgan_efficacy:.4f}\")\n",
    "print(f\"  AUC (Real): {cgan_auc_real:.4f}, AUC (Synthetic): {cgan_auc_synth:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run remaining seeds (123, 456) for GAN\n",
    "gan_results = []\n",
    "\n",
    "for seed in [123, 456]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running GAN with seed {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, preprocessor_seed = prepare_data(df, seed=seed)\n",
    "    \n",
    "    generator, g_losses, d_losses = train_gan(\n",
    "        X_train, preprocessor_seed,\n",
    "        latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "        batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR,\n",
    "        n_critic=N_CRITIC, lambda_gp=LAMBDA_GP,\n",
    "        lambda_corr=LAMBDA_CORR, lambda_special=LAMBDA_SPECIAL,\n",
    "        temperature=TEMPERATURE, dropout=DROPOUT, pac=PAC,\n",
    "        seed=seed, save_dir=f'plots_v2/gan'\n",
    "    )\n",
    "    \n",
    "    plot_training_losses(g_losses, d_losses, \n",
    "                        title=f\"GAN Training Losses (Seed {seed})\",\n",
    "                        save_path=f'plots_v2/gan/losses_seed{seed}.png')\n",
    "    \n",
    "    X_synthetic = generate_synthetic_data(generator, len(X_train), LATENT_DIM, device)\n",
    "    y_synthetic = y_train.copy()\n",
    "    \n",
    "    plot_feature_distributions(X_train, X_synthetic, preprocessor_seed,\n",
    "                              title=f\"GAN Feature Distributions (Seed {seed})\",\n",
    "                              save_path=f'plots_v2/gan/features_seed{seed}.png')\n",
    "    \n",
    "    detection_auc, detection_std = compute_detection_metric(X_train, X_synthetic, seed=seed)\n",
    "    efficacy, auc_real, auc_synth = compute_efficacy_metric(\n",
    "        X_train, y_train, X_synthetic, y_synthetic, X_test, y_test, seed=seed\n",
    "    )\n",
    "    \n",
    "    gan_results.append({\n",
    "        'seed': seed,\n",
    "        'detection_auc': detection_auc,\n",
    "        'efficacy': efficacy\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nGAN Results (Seed {seed}):\")\n",
    "    print(f\"  Detection AUC: {detection_auc:.4f}\")\n",
    "    print(f\"  Efficacy Ratio: {efficacy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run remaining seeds (123, 456) for cGAN\n",
    "cgan_results = []\n",
    "\n",
    "for seed in [123, 456]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running cGAN with seed {seed}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, preprocessor_seed = prepare_data(df, seed=seed)\n",
    "    \n",
    "    cgan_generator, cgan_g_losses, cgan_d_losses = train_cgan(\n",
    "        X_train, y_train, preprocessor_seed,\n",
    "        latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM,\n",
    "        batch_size=BATCH_SIZE, epochs=EPOCHS, lr=LR,\n",
    "        n_critic=N_CRITIC, lambda_gp=LAMBDA_GP,\n",
    "        lambda_corr=LAMBDA_CORR, lambda_special=LAMBDA_SPECIAL,\n",
    "        temperature=TEMPERATURE, dropout=DROPOUT, pac=PAC,\n",
    "        seed=seed, save_dir=f'plots_v2/cgan'\n",
    "    )\n",
    "    \n",
    "    plot_training_losses(cgan_g_losses, cgan_d_losses,\n",
    "                        title=f\"cGAN Training Losses (Seed {seed})\",\n",
    "                        save_path=f'plots_v2/cgan/losses_seed{seed}.png')\n",
    "    \n",
    "    label_ratios = np.bincount(y_train) / len(y_train)\n",
    "    X_synthetic_cgan, y_synthetic_cgan = generate_conditional_synthetic_data(\n",
    "        cgan_generator, len(X_train), LATENT_DIM, label_ratios, device\n",
    "    )\n",
    "    \n",
    "    plot_feature_distributions(X_train, X_synthetic_cgan, preprocessor_seed,\n",
    "                              title=f\"cGAN Feature Distributions (Seed {seed})\",\n",
    "                              save_path=f'plots_v2/cgan/features_seed{seed}.png')\n",
    "    \n",
    "    cgan_detection_auc, cgan_detection_std = compute_detection_metric(X_train, X_synthetic_cgan, seed=seed)\n",
    "    cgan_efficacy, _, _ = compute_efficacy_metric(\n",
    "        X_train, y_train, X_synthetic_cgan, y_synthetic_cgan, X_test, y_test, seed=seed\n",
    "    )\n",
    "    \n",
    "    cgan_results.append({\n",
    "        'seed': seed,\n",
    "        'detection_auc': cgan_detection_auc,\n",
    "        'efficacy': cgan_efficacy\n",
    "    })\n",
    "    \n",
    "    print(f\"\\ncGAN Results (Seed {seed}):\")\n",
    "    print(f\"  Detection AUC: {cgan_detection_auc:.4f}\")\n",
    "    print(f\"  Efficacy Ratio: {cgan_efficacy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY (CTGAN-Style)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Note: seed 42 results need to be added from the first run\n",
    "# This cell assumes you've run all experiments\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "print(f\"  Hidden Dim: {HIDDEN_DIM}, Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  PAC: {PAC}, N_CRITIC: {N_CRITIC}, LR: {LR}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"Results per seed will be printed after running all experiments.\")\n",
    "print(\"Target: Detection AUC < 0.70, Efficacy > 0.85\")\n",
    "print(\"-\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
